{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20203c0f",
   "metadata": {},
   "source": [
    "# I. Project Team Members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3d7f4",
   "metadata": {},
   "source": [
    "| Prepared by | Email | Prepared for |\n",
    "| :-: | :-: | :-: |\n",
    "| **_Your Name_** | _Your Email_ | **_Project Name_** |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b05cd469",
   "metadata": {},
   "source": [
    "# II. Notebook Target Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47bae1d2",
   "metadata": {},
   "source": [
    "_Insert Text Here_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3213f42d",
   "metadata": {},
   "source": [
    "# III. Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb5c3810",
   "metadata": {},
   "source": [
    "## III.A. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from hyperopt import fmin, tpe, space_eval, Trials, STATUS_OK\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7291e85b",
   "metadata": {},
   "source": [
    "## III.B. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f425995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../../data/processed/X_train_woe.pkl')\n",
    "X_test = pd.read_pickle('../../data/processed/X_test_woe.pkl')\n",
    "y_train = pd.read_pickle('../../data/processed/y_train.pkl')\n",
    "y_test = pd.read_pickle('../../data/processed/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3fa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f59e32c9",
   "metadata": {},
   "source": [
    "# IV. Models Training and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0a5756",
   "metadata": {},
   "source": [
    "## IV.A. Data Shape Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3389b1bf",
   "metadata": {},
   "source": [
    "## IV.B. Data Information Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe621948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce34b86f",
   "metadata": {},
   "source": [
    "## IV.C. Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stamp():\n",
    "    return datetime.now()\n",
    "\n",
    "\n",
    "def create_logger():\n",
    "    return {\n",
    "        \"model_name\": [],\n",
    "        \"model_uid\": [],\n",
    "        \"training_time\": [],\n",
    "        \"training_date\": [],\n",
    "        \"performance\": [],\n",
    "        \"f1_score_avg\": [],\n",
    "        \"data_configurations\": []\n",
    "    }\n",
    "\n",
    "\n",
    "def training_log_updater(current_log, log_path):\n",
    "    try:\n",
    "        with open(log_path, 'r') as file:\n",
    "            last_log = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        with open(log_path, 'w') as file:\n",
    "            file.write(\"[]\")\n",
    "        with open(log_path, 'r') as file:\n",
    "            last_log = json.load(file)\n",
    "    last_log.append(current_log)\n",
    "    with open(log_path, 'w') as file:\n",
    "        json.dump(last_log, file)\n",
    "    return last_log\n",
    "\n",
    "\n",
    "def model_training_and_evaluation(models_list, model_prefix, X_train, y_train, X_test, y_test, data_configuration, log_path):\n",
    "    def check_log_length(log_path):\n",
    "        try:\n",
    "            with open(log_path, 'r') as file:\n",
    "                logs = json.load(file)\n",
    "                return len(logs)\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "    before_training_len = check_log_length(log_path)\n",
    "    logger = create_logger()\n",
    "    for model in tqdm(models_list):\n",
    "        model_name = model_prefix + \"-\" + model[\"model_name\"]\n",
    "        start_time = time_stamp()\n",
    "        model[\"model_object\"].fit(X_train, y_train)\n",
    "        finished_time = time_stamp()\n",
    "        elapsed_time = (finished_time - start_time).total_seconds()\n",
    "        y_prediction = model[\"model_object\"].predict(X_test)\n",
    "        performance = classification_report(\n",
    "            y_test, y_prediction, output_dict=True)\n",
    "        original_id = str(start_time) + str(finished_time)\n",
    "        hashed_id = hashlib.md5(original_id.encode()).hexdigest()\n",
    "        model[\"model_uid\"] = hashed_id\n",
    "        logger[\"model_name\"].append(model_name)\n",
    "        logger[\"model_uid\"].append(hashed_id)\n",
    "        logger[\"training_time\"].append(elapsed_time)\n",
    "        logger[\"training_date\"].append(str(start_time))\n",
    "        logger[\"performance\"].append(performance)\n",
    "        logger[\"f1_score_avg\"].append(performance[\"macro avg\"][\"f1-score\"])\n",
    "        logger[\"data_configurations\"].append(data_configuration)\n",
    "    training_log = training_log_updater(logger, log_path)\n",
    "    after_training_len = check_log_length(log_path)\n",
    "    print(f\"Logs Before Training: {before_training_len}\")\n",
    "    print(f\"Logs After Training: {after_training_len}\")\n",
    "    print(f\"Added {after_training_len - before_training_len} new logs.\")\n",
    "    return training_log, models_list\n",
    "\n",
    "\n",
    "def training_log_to_df_converter(training_log):\n",
    "    all_training_logs_df = pd.DataFrame()\n",
    "    for log in tqdm(training_log):\n",
    "        individual_log_df = pd.DataFrame(log)\n",
    "        performance_df = pd.json_normalize(individual_log_df[\"performance\"])\n",
    "        individual_log_df = pd.concat([individual_log_df.drop(\n",
    "            \"performance\", axis=1), performance_df], axis=1)\n",
    "        all_training_logs_df = pd.concat(\n",
    "            [all_training_logs_df, individual_log_df])\n",
    "    all_training_logs_df.sort_values([\"f1_score_avg\", \"training_time\"], ascending=[\n",
    "                                     False, True], inplace=True)\n",
    "    all_training_logs_df.reset_index(inplace=True, drop=True)\n",
    "    return all_training_logs_df\n",
    "\n",
    "\n",
    "def best_model_finder(all_training_logs_df, models_list):\n",
    "    model_object = None\n",
    "    best_model_info = all_training_logs_df.iloc[0]\n",
    "    for configuration_data in models_list:\n",
    "        for model_data in models_list[configuration_data]:\n",
    "            if model_data[\"model_uid\"] == best_model_info[\"model_uid\"]:\n",
    "                model_object = model_data[\"model_object\"]\n",
    "                break\n",
    "    if model_object == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    return model_object\n",
    "\n",
    "def tuned_model_finder(models_list_tuned, tuning_method):\n",
    "    for model in models_list_tuned:\n",
    "        if tuning_method in model[\"model_name\"]:\n",
    "            return model[\"model_object\"]\n",
    "    print(f\"No model found that was tuned with {tuning_method}\")\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69eefd12",
   "metadata": {},
   "source": [
    "## IV.D. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68192643",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_baseline = LogisticRegression(random_state=777)\n",
    "ebm_baseline = ExplainableBoostingClassifier(random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9af385",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = {\n",
    "    \"vanilla\": [\n",
    "        {\"model_name\": log_reg_baseline.__class__.__name__,\n",
    "            \"model_object\": log_reg_baseline, \"model_uid\": \"\"},\n",
    "        {\"model_name\": ebm_baseline.__class__.__name__,\n",
    "            \"model_object\": ebm_baseline, \"model_uid\": \"\"}\n",
    "    ],\n",
    "    \"smote\": [\n",
    "        {\"model_name\": log_reg_baseline.__class__.__name__,\n",
    "            \"model_object\": log_reg_baseline, \"model_uid\": \"\"},\n",
    "        {\"model_name\": ebm_baseline.__class__.__name__,\n",
    "            \"model_object\": ebm_baseline, \"model_uid\": \"\"},\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fde862-2df9-475f-a2c6-78bbba00227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a98cf1f",
   "metadata": {},
   "source": [
    "### IV.D.1. Vanilla Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853db323",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_vanilla = model_training_and_evaluation(\n",
    "    models_list[\"vanilla\"],\n",
    "    \"baseline_model\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"vanilla\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a23e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03126b44",
   "metadata": {},
   "source": [
    "### IV.D.2. Sampling Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_smote = model_training_and_evaluation(\n",
    "    models_list[\"smote\"],\n",
    "    \"smote_model\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"smote\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ac844f9",
   "metadata": {},
   "source": [
    "## IV.E. Models Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d259e2f-1538-4548-9e7f-5f4e9871be78",
   "metadata": {},
   "source": [
    "### IV.E.1. Benchmark Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance that a model would achieve if it always predicted the most common label.\n",
    "benchmark = y_train.value_counts(normalize=True)[0]\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f14d9-88d7-4ba6-b116-929db1b76c89",
   "metadata": {},
   "source": [
    "### IV.E.2. Baseline Base Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2eaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_logs_df = training_log_to_df_converter(training_log)\n",
    "all_training_logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_best_model = best_model_finder(all_training_logs_df, models_list)\n",
    "baseline_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_info = all_training_logs_df.iloc[0]\n",
    "print(\"Best model configuration:\", best_model_info[\"data_configurations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c40f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_dataframe(model, X_train, y_train, X_test, y_test):\n",
    "    train_prediction = model.predict(X_train)\n",
    "    test_prediction = model.predict(X_test)\n",
    "    train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def get_prediction_metrics(y_true, y_pred, y_probs):\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_probs)\n",
    "        metrics = {\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1-score\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"auc_roc\": auc_roc\n",
    "        }\n",
    "        return metrics\n",
    "    train_metrics = get_prediction_metrics(\n",
    "        y_train, train_prediction, train_probs)\n",
    "    train_metrics[\"dataset\"] = \"Train\"\n",
    "    test_metrics = get_prediction_metrics(y_test, test_prediction, test_probs)\n",
    "    test_metrics[\"dataset\"] = \"Test\"\n",
    "    return pd.DataFrame([train_metrics, test_metrics])\n",
    "\n",
    "\n",
    "def display_confusion_matrix(model, X_train, y_train, X_test, y_test):\n",
    "    train_prediction = model.predict(X_train)\n",
    "    test_prediction = model.predict(X_test)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_train, train_prediction, ax=ax[0])\n",
    "    ax[0].set_title(\"Train Confusion Matrix\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, test_prediction, ax=ax[1])\n",
    "    ax[1].set_title(\"Test Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train_vs_test_error(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    train_error = 1 - accuracy_score(y_train, y_pred_train)\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred_test)\n",
    "    bars = plt.bar([\"Train Error\", \"Test Error\"], [train_error, test_error])\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Train vs Test Error\")\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005,\n",
    "                 round(yval, 2), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_prob)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fpr_train, tpr_train,\n",
    "             label=f\"Train AUC: {roc_auc_score(y_train, y_pred_train_prob):.2f}\")\n",
    "    plt.plot(fpr_test, tpr_test,\n",
    "             label=f\"Test AUC: {roc_auc_score(y_test, y_pred_test_prob):.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_learning_curve(model, X, y, cv=50):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), n_jobs=-1)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-',\n",
    "             color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-',\n",
    "             color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0718cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9116f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    baseline_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(baseline_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(baseline_best_model, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bb2fc6d",
   "metadata": {},
   "source": [
    "### IV.E.3. Export Baseline Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../models/baseline_best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(baseline_best_model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de0ed9a4",
   "metadata": {},
   "source": [
    "## IV.F. Hyperparameters Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be189d67",
   "metadata": {},
   "source": [
    "### IV.F.1. Hyperparameters List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882ba16",
   "metadata": {},
   "source": [
    "#### IV.F.1.A. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7713f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_hyperparams = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5663b2-0c93-4dfb-b23d-79e98eac1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=777),\n",
    "    log_reg_hyperparams,\n",
    "    n_jobs=-1,\n",
    "    verbose=420,\n",
    "    scoring='f1_macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc81ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_from_grid = log_reg_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30add73-650f-41b2-848a-9f5e56568dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list[\"fine-tuned\"] = [{\"model_name\": \"GridSearchBest-LogisticRegression\",\n",
    "                              \"model_object\": best_estimator_from_grid, \"model_uid\": \"\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aaa7f",
   "metadata": {},
   "source": [
    "#### IV.F.1.B. Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_space = {\n",
    "    'penalty': hyperopt.hp.choice('penalty', ['l1', 'l2']),\n",
    "    'C': hyperopt.hp.loguniform('C', np.log(1e-4), np.log(1e4)),\n",
    "    'solver': 'liblinear'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    classifier = LogisticRegression(**params, random_state=777)\n",
    "    score = cross_val_score(classifier, X_train,\n",
    "                            y_train, cv=5, scoring='f1_macro').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=log_reg_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "best_params = space_eval(log_reg_space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_log_reg = LogisticRegression(**best_params, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list[\"fine-tuned\"].append({\"model_name\": \"BayesOpt-LogisticRegression\",\n",
    "                                  \"model_object\": optimal_log_reg, \"model_uid\": \"\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7943a5bb",
   "metadata": {},
   "source": [
    "### IV.F.2. Best Model Hyperparameter Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log, models_list_tuned = model_training_and_evaluation(\n",
    "    models_list[\"fine-tuned\"],\n",
    "    \"tuned_model\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"tuned\",\n",
    "    '../../models/logs/training_log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c59ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc10a9-fdee-4c0b-951a-5ff793172a27",
   "metadata": {},
   "source": [
    "### IV.F.3. Hyperparameter-tuned Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f99ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_logs_df_tuned = training_log_to_df_converter(training_log)\n",
    "all_training_logs_df_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd33386",
   "metadata": {},
   "source": [
    "#### IV.F.3.A. Grid Searched Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_tuned = {\"fine-tuned\": models_list_tuned}\n",
    "tuned_best_model = tuned_model_finder(\n",
    "    models_dict_tuned[\"fine-tuned\"], \"GridSearchBest\")\n",
    "tuned_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab61f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38be23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56320b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(tuned_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e821a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(tuned_best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c807ae",
   "metadata": {},
   "source": [
    "#### IV.F.3.B. Bayesian Searched Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_tuned = {\"fine-tuned\": models_list_tuned}\n",
    "tuned_best_model = tuned_model_finder(\n",
    "    models_dict_tuned[\"fine-tuned\"], \"BayesOpt\")\n",
    "tuned_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = get_metrics_dataframe(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_vs_test_error(\n",
    "    tuned_best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(tuned_best_model, X_train,\n",
    "               y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curve(tuned_best_model, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c44a6b",
   "metadata": {},
   "source": [
    "### IV.F.4. Export Hyperparameter-tuned Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../models/tuned_best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(tuned_best_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
